{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9273bf97-14ff-4fb1-ad8e-18c338661d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an LLM chat model only with OpenVINO (supports only the stateful, KV-caching enabled LLM models)\n",
    "#  - Without 'optimum-intel', 'PyTorch' and HF-Tokenizers.\n",
    "#  This program uses sampling method to generate the output text.\n",
    "\n",
    "import numpy as np\n",
    "from transformers import LlamaTokenizer\n",
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9ede4920-2d81-4e22-aa6f-9f61559e0eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model...finished.\n",
      "<CompiledModel:\n",
      "inputs[\n",
      "<ConstOutput: names[input_ids] shape[?,?] type: i32>,\n",
      "<ConstOutput: names[attention_mask] shape[?,?] type: i32>,\n",
      "<ConstOutput: names[position_ids] shape[?,?] type: i32>,\n",
      "<ConstOutput: names[use_cache] shape[] type: i32>,\n",
      "<ConstOutput: names[output_attentions] shape[] type: i32>,\n",
      "<ConstOutput: names[output_hidden_states] shape[] type: i32>,\n",
      "<ConstOutput: names[return_dict] shape[] type: i32>\n",
      "]\n",
      "outputs[\n",
      "<ConstOutput: names[6430] shape[?,?,65536] type: f32>\n",
      "]>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"novelai/nerdstash-tokenizer-v1\", additional_special_tokens=['▁▁'])\n",
    "\n",
    "device = 'CPU'\n",
    "ov_config={\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"./cache\"}\n",
    "print('Compiling the model...', end='', flush=True)\n",
    "#compiled_model = ov.compile_model('openvino_model_int8.xml', device, ov_config)\n",
    "compiled_model = ov.compile_model('openvino_model_int8_no_kv_out.xml', device, ov_config)\n",
    "infer_request = compiled_model.create_infer_request()\n",
    "print('finished.')\n",
    "\n",
    "print(compiled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cef44057-c9aa-449a-9246-a7ea4d434638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(user_query, inputs=\"\", sep=\"\\n\\n### \"):\n",
    "    sys_msg = \"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\"\n",
    "    p = sys_msg\n",
    "    roles = [\"指示\", \"応答\"]\n",
    "    msgs = [\": \\n\" + user_query, \": \"]\n",
    "    if inputs:\n",
    "        roles.insert(1, \"入力\")\n",
    "        msgs.insert(1, \": \\n\" + inputs)\n",
    "    for role, msg in zip(roles, msgs):\n",
    "        p += sep + role + msg\n",
    "    return p\n",
    "\n",
    "# Infer with prompt without any additional input\n",
    "user_inputs = {\n",
    "    \"user_query\": \"VR とはどのようなものですか？\",\n",
    "    \"inputs\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ee3a0-06b5-4c96-a442-56dc216b3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(**user_inputs)\n",
    "\n",
    "tokens = tokenizer(\n",
    "    prompt, \n",
    "    add_special_tokens=False, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Tokenize the input text (text -> token IDs)\n",
    "# - The model input for the 1st iteration\n",
    "num_tokens     = tokens.input_ids.shape[-1]\n",
    "input_ids      = tokens.input_ids\n",
    "attention_mask = tokens.attention_mask\n",
    "position       = num_tokens\n",
    "position_ids   = np.array([range(position)], dtype=np.int64)\n",
    "beam_idx       = np.array([0], dtype=np.int32)\n",
    "\n",
    "num_max_token_for_generation = 20\n",
    "generated_text_ids = []\n",
    "prev_output = ''\n",
    "\n",
    "#past_key_values = ov.Tensor(type=ov.Type.f32 , shape=(32,2,1,32,0,128))\n",
    "past_key_value = ov.Tensor(type=ov.Type.f32 , shape=(1,32,0,128))\n",
    "past_key_values = tuple([ (past_key_value, past_key_value) for _ in range(32) ])\n",
    "\n",
    "print(len(past_key_values), len(past_key_values[0]), past_key_values[0][1].shape)\n",
    "\n",
    "# generate lists that contains kv_cache output node names\n",
    "o_past_kv_names = []\n",
    "o_hidden_states_names = []\n",
    "for n in range(32):\n",
    "    past_kv_name = str(7204 + n)\n",
    "    o_past_kv_names.append(past_kv_name)\n",
    "    hidden_states_name = str(7172 + n)\n",
    "    o_hidden_states_names.append(hidden_states_name)\n",
    "\n",
    "i_past_kv_names = []\n",
    "for n in range(64):\n",
    "    past_kv_name = str(42 + n)\n",
    "    i_past_kv_names.append(past_kv_name)\n",
    "\n",
    "#print(past_kv_names)\n",
    "print(tokens.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b240fb-d016-4e0a-8431-bc24192fd3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*** Start inferencing')\n",
    "\n",
    "infer_request.reset_state()                                     # Initialize model internal state\n",
    "for i in range(num_max_token_for_generation):\n",
    "\n",
    "    # Run inference (to generate the logits for the next word prediction)\n",
    "    #print('inf', i, ' : ', end='', flush=True)\n",
    "    inputs={'input_ids'            : input_ids,\n",
    "            'attention_mask'       : attention_mask,\n",
    "            'position_ids'         : position_ids,\n",
    "            'use_cache'            : 1,\n",
    "            'output_attentions'    : 0,\n",
    "            'output_hidden_states' : 0,\n",
    "            'return_dict'          : 0,\n",
    "    }\n",
    "    \"\"\"\n",
    "    for n in range(32):\n",
    "        input_name = str(42 + n)\n",
    "        inputs[input_name] = past_key_values[n][0]\n",
    "        input_name = str(42 + 32 + n)\n",
    "        inputs[input_name] = past_key_values[n][1]\n",
    "    \"\"\"\n",
    "    response = infer_request.infer(inputs)\n",
    "\n",
    "    #print(response)\n",
    "    #print(response[o_past_kv_names[0]].shape)  # [1, 32, num_seq, 128]\n",
    "    #print(response['hidden_states.1'].shape) # [1, num_seq,4096]\n",
    "    #(1, 53, 4096)\n",
    "    #(1, 53, 4096)\n",
    "    #(1, 32, 53, 53)\n",
    "\n",
    "    #logits_name = '7237'\n",
    "    logits_name = '6430'\n",
    "    logits = response[logits_name][0, -1, :].ravel()\n",
    "    sampled_token_id = np.argmax(logits)\n",
    "    #print(logits, logits.shape, sampled_id)\n",
    "\n",
    "    if sampled_token_id == tokenizer.eos_token_id:\n",
    "        print('\\n*** EOS token detected.')\n",
    "        break\n",
    "    generated_text_ids = np.append(generated_text_ids, sampled_token_id)  # Append the predicted word to the bottom of the generated text ID array\n",
    "    output_text = tokenizer.decode(generated_text_ids)              # Decode and generate the text from the array of token IDs\n",
    "    print(output_text[len(prev_output):], end='', flush=True)       # Print only the last generated word\n",
    "    #print()\n",
    "    prev_output = output_text\n",
    "    #print(output_text)\n",
    "\n",
    "    #input_ids = np.append(input_ids, [[sampled_token_id]], axis=1)\n",
    "    #attention_mask = np.append(attention_mask, [[1]], axis=1)\n",
    "    #position_ids = np.append(position_ids, [[position]], axis=1)\n",
    "    \"\"\"\n",
    "    past_key_values = []\n",
    "    for n in range(32):\n",
    "        past_key_value      = response[o_past_kv_names[n]] #  1,32,seq_len,seq_len   .reshape(1,32,-1,128)\n",
    "        hidden_states_value = response[o_hidden_states_names[n]].reshape(1, 32, -1, 128)\n",
    "        #past_key_values.append((past_key_value, hidden_states_value))\n",
    "        past_key_values.append((past_key_value, hidden_states_value))\n",
    "    #print(response[o_past_kv_names[0]].shape, response[o_hidden_states_names[0]].shape)\n",
    "    print(past_key_values[0][0].shape, past_key_values[0][1].shape)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    input_ids      = np.array([[sampled_token_id]], dtype=np.int64)\n",
    "    attention_mask = np.array([[1]], dtype=np.int64)\n",
    "    position_ids   = np.array([[position]], dtype=np.int64)\n",
    "    \"\"\"\n",
    "    input_ids      = np.append(input_ids, [[sampled_token_id]], axis=1)\n",
    "    attention_mask = np.append(attention_mask, [[1]], axis=1)\n",
    "    position_ids   = np.append(position_ids, [[position]], axis=1)\n",
    "    #\"\"\"\n",
    "    position      += 1\n",
    "\n",
    "print(f'\\n\\n*** Completed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
