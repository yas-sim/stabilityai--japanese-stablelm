{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c571bf74-b168-454b-a410-01028b79daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, TextStreamer\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "import openvino as ov\n",
    "\n",
    "model_id = 'stabilityai/japanese-stablelm-base-gamma-7b'\n",
    "model_vendor, model_name = model_id.split('/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd01057",
   "metadata": {},
   "source": [
    "## モデル変換 (OpenVINOモデル生成)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6305b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nncf\n",
    "\n",
    "if not os.path.exists(f'{model_name}/INT4'):\n",
    "    ov_model=OVModelForCausalLM.from_pretrained(model_id, export=True, compile=False, load_in_8bit=False)\n",
    "    compressed_model = nncf.compress_weights(ov_model.half()._original_model, mode=nncf.CompressWeightsMode.INT4_ASYM, group_size=128, ratio=0.8)\n",
    "    os.makedirs(f'{model_name}/INT4')\n",
    "    ov.save_model(compressed_model, f'{model_name}/INT4/openvino_model.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf2101e",
   "metadata": {},
   "source": [
    "## OpenVINOモデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b2b82c-5fe2-4b75-858e-76e82fd643c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "ov_model = OVModelForCausalLM.from_pretrained(\n",
    "    model_id = f'{model_name}/INT4',\n",
    "    device='CPU',\n",
    "    ov_config={\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"./cache\"},\n",
    "    config=AutoConfig.from_pretrained(model_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e188e5b8-f5db-4397-a3ec-f4e7f0da2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(user_query, inputs=\"\", sep=\"\\n\\n### \"):\n",
    "    sys_msg = \"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\"\n",
    "    p = sys_msg\n",
    "    roles = [\"指示\", \"応答\"]\n",
    "    msgs = [\": \\n\" + user_query, \": \"]\n",
    "    if inputs:\n",
    "        roles.insert(1, \"入力\")\n",
    "        msgs.insert(1, \": \\n\" + inputs)\n",
    "    for role, msg in zip(roles, msgs):\n",
    "        p += sep + role + msg\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "898808ef-bc2d-4573-8315-7f89a89e2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer with prompt without any additional input\n",
    "user_inputs = {\n",
    "    \"user_query\": \"VR とはどのようなものですか？\",\n",
    "    \"inputs\": \"\"\n",
    "}\n",
    "prompt = build_prompt(**user_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1984265-1c76-4d34-a755-71645cd64cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Prompt:\n",
      "以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
      "\n",
      "### 指示: \n",
      "VR とはどのようなものですか？\n",
      "\n",
      "### 応答: \n",
      "-------------------------\n",
      "\n",
      "Virtual Reality （バーチャル・リアリティ）とは仮想現実のことだ。コンピュータが生成した視覚的または音声情報を使用して、ユーザの感覚器官や神経系を弱める方法で人間の意識を変更することを言う。例えばゲーム内で自分自身が操作キャラクターの体になって行動出来たり、映画館で見られる3D映像なんかがその典型例だね！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'** Prompt:\\n{prompt}\\n-------------------------')\n",
    "input_tokens = tokenizer(prompt, return_tensors='pt', add_special_tokens=False)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "response = ov_model.generate(**input_tokens, \n",
    "                             pad_token_id=tokenizer.eos_token_id,\n",
    "                             eos_token_id=tokenizer.eos_token_id,\n",
    "                             max_new_tokens=300,\n",
    "                             num_return_sequences=1,\n",
    "                             temperature=1.0,\n",
    "                             do_sample=True,\n",
    "                             top_k=5,\n",
    "                             top_p=0.90,\n",
    "                             repetition_penalty=1.2,\n",
    "                             streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e62e8-f815-44b8-9c66-7d60d0374c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84474520-eb60-4db8-b676-2564f91fe77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !optimum-cli export openvino -m stabilityai/japanese-stablelm-base-gamma-7b --trust-remote-code --weight-format int4_asym_g64 --disable-stateful japanese-stablelm-base-gamma-7b/INT4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
