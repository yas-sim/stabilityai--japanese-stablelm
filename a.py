a=['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_auto_class', '_autoset_attn_implementation', '_backward_compatibility_gradient_checkpointing', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_check_and_enable_flash_attn_2', '_check_and_enable_sdpa', '_compiled_call_impl', '_convert_head_mask_to_5d', '_copy_lm_head_original_to_resized', '_create_repo', '_dispatch_accelerate_model', '_expand_inputs_for_generation', '_extract_past_from_model_output', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_from_config', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_candidate_generator', '_get_decoder_start_token_id', '_get_files_timestamps', '_get_generation_mode', '_get_logits_processor', '_get_logits_warper', '_get_name', '_get_no_split_modules', '_get_resized_embeddings', '_get_resized_lm_head', '_get_stopping_criteria', '_hf_peft_config_loaded', '_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_init_weights', '_initialize_weights', '_is_full_backward_hook', '_is_hf_initialized', '_keep_in_fp32_modules', '_keep_in_fp32_modules', '_keys_to_ignore_on_load_missing', '_keys_to_ignore_on_load_unexpected', '_keys_to_ignore_on_save', '_load_from_state_dict', '_load_pretrained_model', '_load_pretrained_model_low_mem', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_initialize_input_ids_for_generation', '_maybe_warn_non_full_backward_hook', '_merge_criteria_processor_list', '_modules', '_named_members', '_no_split_modules', '_non_persistent_buffers_set', '_parameters', '_prepare_attention_mask_for_generation', '_prepare_decoder_input_ids_for_generation', '_prepare_encoder_decoder_kwargs_for_generation', '_prepare_model_inputs', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_reorder_cache', '_replicate_for_data_parallel', '_resize_token_embeddings', '_save_to_state_dict', '_set_default_torch_dtype', '_set_gradient_checkpointing', '_skip_keys_device_placement', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_supports_cache_class', '_supports_flash_attn_2', '_supports_sdpa', '_temporary_reorder_cache', '_tie_encoder_decoder_weights', '_tie_or_clone_weights', '_tied_weights_keys', '_update_model_kwargs_for_generation', '_upload_modified_files', '_validate_generated_length', '_validate_model_class', '_validate_model_kwargs', '_version', '_wrapped_call_impl', 'active_adapter', 'active_adapters', 'add_adapter', 'add_memory_hooks', 'add_model_tags', 'add_module', 'apply', 'assisted_decoding', 'base_model', 'base_model_prefix', 'beam_sample', 'beam_search', 'bfloat16', 'buffers', 'call_super_init', 'can_generate', 'children', 'compile', 'compute_transition_scores', 'config', 'config_class', 'constrained_beam_search', 'contrastive_search', 'cpu', 'create_extended_attention_mask_for_decoder', 'cuda', 'device', 'disable_adapters', 'disable_input_require_grads', 'double', 'dtype', 'dummy_inputs', 'dump_patches', 'embed_out', 'enable_adapters', 'enable_input_require_grads', 'estimate_tokens', 'eval', 'extra_repr', 'float', 'floating_point_ops', 'forward', 'framework', 'from_pretrained', 'generate', 'generation_config', 'get_adapter_state_dict', 'get_buffer', 'get_extended_attention_mask', 'get_extra_state', 'get_head_mask', 'get_input_embeddings', 'get_memory_footprint', 'get_output_embeddings', 'get_parameter', 'get_position_embeddings', 'get_submodule', 'gradient_checkpointing_disable', 'gradient_checkpointing_enable', 'greedy_search', 'group_beam_search', 'half', 'init_weights', 'invert_attention_mask', 'ipu', 'is_gradient_checkpointing', 'is_loaded_in_4bit', 'is_loaded_in_8bit', 'is_parallelizable', 'load_adapter', 'load_state_dict', 'main_input_name', 'model_tags', 'modules', 'name_or_path', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_parameters', 'parameters', 'post_init', 'prepare_inputs_for_generation', 'prune_heads', 'push_to_hub', 'register_backward_hook', 'register_buffer', 'register_for_auto_class', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_memory_hooks_state', 'resize_position_embeddings', 'resize_token_embeddings', 'retrieve_modules_from_names', 'reverse_bettertransformer', 'sample', 'save_pretrained', 'set_adapter', 'set_extra_state', 'set_input_embeddings', 'set_output_embeddings', 'share_memory', 'state_dict', 'supports_gradient_checkpointing', 'tie_weights', 'to', 'to_bettertransformer', 'to_empty', 'train', 'training', 'transformer', 'type', 'warn_if_padding_and_no_attention_mask', 'warnings_issued', 'xpu', 'zero_grad']
for b in a:
    print(b)

"""
T_destination
__annotations__
__call__
__class__
__delattr__
__dict__
__dir__
__doc__
__eq__
__format__
__ge__
__getattr__
__getattribute__
__getstate__
__gt__
__hash__
__init__
__init_subclass__
__le__
__lt__
__module__
__ne__
__new__
__reduce__
__reduce_ex__
__repr__
__setattr__
__setstate__
__sizeof__
__str__
__subclasshook__
__weakref__
_apply
_auto_class
_autoset_attn_implementation
_backward_compatibility_gradient_checkpointing
_backward_hooks
_backward_pre_hooks
_buffers
_call_impl
_check_and_enable_flash_attn_2
_check_and_enable_sdpa
_compiled_call_impl
_convert_head_mask_to_5d
_copy_lm_head_original_to_resized
_create_repo
_dispatch_accelerate_model
_expand_inputs_for_generation
_extract_past_from_model_output
_forward_hooks
_forward_hooks_always_called
_forward_hooks_with_kwargs
_forward_pre_hooks
_forward_pre_hooks_with_kwargs
_from_config
_get_backward_hooks
_get_backward_pre_hooks
_get_candidate_generator
_get_decoder_start_token_id
_get_files_timestamps
_get_generation_mode
_get_logits_processor
_get_logits_warper
_get_name
_get_no_split_modules
_get_resized_embeddings
_get_resized_lm_head
_get_stopping_criteria
_hf_peft_config_loaded
_hook_rss_memory_post_forward
_hook_rss_memory_pre_forward
_init_weights
_initialize_weights
_is_full_backward_hook
_is_hf_initialized
_keep_in_fp32_modules
_keep_in_fp32_modules
_keys_to_ignore_on_load_missing
_keys_to_ignore_on_load_unexpected
_keys_to_ignore_on_save
_load_from_state_dict
_load_pretrained_model
_load_pretrained_model_low_mem
_load_state_dict_post_hooks
_load_state_dict_pre_hooks
_maybe_initialize_input_ids_for_generation
_maybe_warn_non_full_backward_hook
_merge_criteria_processor_list
_modules
_named_members
_no_split_modules
_non_persistent_buffers_set
_parameters
_prepare_attention_mask_for_generation
_prepare_decoder_input_ids_for_generation
_prepare_encoder_decoder_kwargs_for_generation
_prepare_model_inputs
_register_load_state_dict_pre_hook
_register_state_dict_hook
_reorder_cache
_replicate_for_data_parallel
_resize_token_embeddings
_save_to_state_dict
_set_default_torch_dtype
_set_gradient_checkpointing
_skip_keys_device_placement
_slow_forward
_state_dict_hooks
_state_dict_pre_hooks
_supports_cache_class
_supports_flash_attn_2
_supports_sdpa
_temporary_reorder_cache
_tie_encoder_decoder_weights
_tie_or_clone_weights
_tied_weights_keys
_update_model_kwargs_for_generation
_upload_modified_files
_validate_generated_length
_validate_model_class
_validate_model_kwargs
_version
_wrapped_call_impl
active_adapter
active_adapters
add_adapter
add_memory_hooks
add_model_tags
add_module
apply
assisted_decoding
base_model
base_model_prefix
beam_sample
beam_search
bfloat16
buffers
call_super_init
can_generate
children
compile
compute_transition_scores
config
config_class
constrained_beam_search
contrastive_search
cpu
create_extended_attention_mask_for_decoder
cuda
device
disable_adapters
disable_input_require_grads
double
dtype
dummy_inputs
dump_patches
embed_out
enable_adapters
enable_input_require_grads
estimate_tokens
eval
extra_repr
float
floating_point_ops
forward
framework
from_pretrained
generate
generation_config
get_adapter_state_dict
get_buffer
get_extended_attention_mask
get_extra_state
get_head_mask
get_input_embeddings
get_memory_footprint
get_output_embeddings
get_parameter
get_position_embeddings
get_submodule
gradient_checkpointing_disable
gradient_checkpointing_enable
greedy_search
group_beam_search
half
init_weights
invert_attention_mask
ipu
is_gradient_checkpointing
is_loaded_in_4bit
is_loaded_in_8bit
is_parallelizable
load_adapter
load_state_dict
main_input_name
model_tags
modules
name_or_path
named_buffers
named_children
named_modules
named_parameters
num_parameters
parameters
post_init
prepare_inputs_for_generation
prune_heads
push_to_hub
register_backward_hook
register_buffer
register_for_auto_class
register_forward_hook
register_forward_pre_hook
register_full_backward_hook
register_full_backward_pre_hook
register_load_state_dict_post_hook
register_module
register_parameter
register_state_dict_pre_hook
requires_grad_
reset_memory_hooks_state
resize_position_embeddings
resize_token_embeddings
retrieve_modules_from_names
reverse_bettertransformer
sample
save_pretrained
set_adapter
set_extra_state
set_input_embeddings
set_output_embeddings
share_memory
state_dict
supports_gradient_checkpointing
tie_weights
to
to_bettertransformer
to_empty
train
training
transformer
type
warn_if_padding_and_no_attention_mask
warnings_issued
xpu
zero_grad
"""