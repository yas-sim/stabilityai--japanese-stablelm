{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9273bf97-14ff-4fb1-ad8e-18c338661d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an LLM chat model only with OpenVINO (supports only the stateful, KV-caching enabled LLM models)\n",
    "#  - Without 'optimum-intel', 'PyTorch' and HF-Tokenizers.\n",
    "#  This program uses sampling method to generate the output text.\n",
    "\n",
    "import numpy as np\n",
    "from transformers import LlamaTokenizer, AutoTokenizer\n",
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9ede4920-2d81-4e22-aa6f-9f61559e0eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model...finished.\n",
      "<CompiledModel:\n",
      "inputs[\n",
      "<ConstOutput: names[input_ids] shape[?,?] type: i32>,\n",
      "<ConstOutput: names[attention_mask, 336] shape[?,?] type: i32>,\n",
      "<ConstOutput: names[position_ids] shape[?,?] type: i32>,\n",
      "<ConstOutput: names[42, key_states.1] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[43] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[44] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[45] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[46] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[47] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[48] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[49] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[50] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[51] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[52] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[53] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[54] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[55] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[56] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[57] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[58] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[59] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[60] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[61] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[62] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[63] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[64] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[65] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[66] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[67] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[68] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[69] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[70] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[71] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[72] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[73] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[74] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[75] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[76] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[77] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[78] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[79] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[80] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[81] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[82] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[83] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[84] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[85] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[86] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[87] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[88] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[89] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[90] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[91] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[92] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[93] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[94] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[95] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[96] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[97] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[98] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[99] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[100] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[101] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[102] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[103] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[104] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[105] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[use_cache] shape[] type: boolean>,\n",
      "<ConstOutput: names[output_attentions] shape[] type: boolean>,\n",
      "<ConstOutput: names[output_hidden_states] shape[] type: boolean>,\n",
      "<ConstOutput: names[return_dict] shape[] type: boolean>\n",
      "]\n",
      "outputs[\n",
      "<ConstOutput: names[171, 5869, 5870, logits] shape[?,?,32000] type: f32>,\n",
      "<ConstOutput: names[446, 107, 520, 490, hidden_states.11, 5804] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[108, 5805, 448, 521, 491, hidden_states.15] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[660, 109, 616, 5806, 690, hidden_states.41] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[5807, 110, 618, 661, 691, hidden_states.45] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[111, 5808, 830, 786, 860, hidden_states.71] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[831, 112, 5809, 861, 788, hidden_states.75] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[113, 1000, 956, 1030, 5810, hidden_states.101] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[1001, 1031, 958, 114, hidden_states.105, 5811] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[5812, 1126, 115, 1170, 1200, hidden_states.131] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[hidden_states.135, 1201, 1128, 1171, 116, 5813] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[hidden_states.161, 117, 5814, 1296, 1340, 1370] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[1371, 118, 1298, 1341, 5815, hidden_states.165] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[1510, 119, 1466, hidden_states.191, 5816, 1540] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[1468, 120, 1511, 5817, 1541, hidden_states.195] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[hidden_states.221, 5818, 121, 1636, 1680, 1710] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[122, hidden_states.225, 1638, 1681, 1711, 5819] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[123, hidden_states.251, 1806, 1850, 1880, 5820] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[hidden_states.255, 124, 1808, 1851, 1881, 5821] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[2020, 125, 2050, 1976, 5822, hidden_states.281] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[126, 1978, 2021, 2051, 5823, hidden_states.285] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[2220, 127, 2146, 2190, 5824, hidden_states.311] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[128, 2148, 2191, 2221, 5825, hidden_states.315] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[129, 2316, 2360, 2390, hidden_states.341, 5826] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[2361, 130, 5827, 2318, 2391, hidden_states.345] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[131, 2486, hidden_states.371, 2530, 2560, 5828] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[5829, 132, 2488, 2531, 2561, hidden_states.375] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[133, 2700, 2656, 2730, 5830, hidden_states.401] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[134, 2658, 2701, 2731, 5831, hidden_states.405] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[2870, 135, 2826, 2900, 5832, hidden_states.431] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[136, 2828, 2871, 2901, hidden_states.435, 5833] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[137, hidden_states.461, 2996, 3040, 3070, 5834] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[hidden_states.465, 138, 3041, 2998, 3071, 5835] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[hidden_states.491, 3240, 139, 3210, 3166, 5836] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[hidden_states.495, 140, 3241, 3168, 3211, 5837] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[5838, 3410, 3380, 141, 3336, hidden_states.521] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[142, 5839, 3411, 3381, 3338, hidden_states.525] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[143, 3506, 3550, 3580, 5840, hidden_states.551] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[144, 3508, hidden_states.555, 3551, 3581, 5841] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[145, 3720, 3676, 5842, 3750, hidden_states.581] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[hidden_states.585, 3721, 146, 3678, 5843, 3751] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[147, 5844, 3846, 3890, 3920, hidden_states.611] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[3921, 148, hidden_states.615, 3848, 3891, 5845] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[149, 4016, hidden_states.641, 4060, 4090, 5846] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[5847, 150, 4018, 4061, hidden_states.645, 4091] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[151, hidden_states.671, 5848, 4186, 4230, 4260] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[4231, 152, 4188, 4261, 5849, hidden_states.675] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[hidden_states.701, 153, 4356, 5850, 4400, 4430] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[4358, 154, 5851, 4401, 4431, hidden_states.705] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[4526, 155, 4570, 5852, 4600, hidden_states.731] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[hidden_states.735, 156, 4528, 4571, 5853, 4601] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[4696, 157, 4740, 4770, 5854, hidden_states.761] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[158, hidden_states.765, 5855, 4698, 4741, 4771] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[159, 4866, 4910, 4940, 5856, hidden_states.791] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[160, 4868, 4911, 4941, 5857, hidden_states.795] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[5858, 5036, 161, 5080, 5110, hidden_states.821] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[162, 5081, 5038, 5111, 5859, hidden_states.825] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[163, hidden_states.851, 5206, 5250, 5860, 5280] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[5208, 164, hidden_states.855, 5251, 5861, 5281] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[5450, 165, 5376, 5420, 5862, hidden_states.881] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[166, 5451, 5378, hidden_states.885, 5421, 5863] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[167, 5546, 5590, 5620, 5864, hidden_states.911] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[168, hidden_states.915, 5621, 5548, 5591, 5865] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[169, hidden_states.941, 5716, 5760, 5866, 5790] shape[?,8,?,128] type: f32>,\n",
      "<ConstOutput: names[170, 5718, 5761, 5867, 5791, hidden_states.945] shape[?,8,?,128] type: f32>\n",
      "]>\n"
     ]
    }
   ],
   "source": [
    "#model_id = 'stabilityai/japanese-stablelm-base-alpha-7b'\n",
    "model_id = 'stabilityai/japanese-stablelm-base-gamma-7b'\n",
    "model_vendor, model_name = model_id.split('/') \n",
    "\n",
    "#tokenizer = LlamaTokenizer.from_pretrained(\"novelai/nerdstash-tokenizer-v1\", additional_special_tokens=['▁▁'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "device = 'CPU'\n",
    "ov_config={\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"./cache\"}\n",
    "print('Compiling the model...', end='', flush=True)\n",
    "#compiled_model = ov.compile_model('stateless-int8/openvino_model.xml', device, ov_config)\n",
    "compiled_model = ov.compile_model('openvino_model_int8.xml', device, ov_config)\n",
    "infer_request = compiled_model.create_infer_request()\n",
    "print('finished.')\n",
    "\n",
    "print(compiled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cef44057-c9aa-449a-9246-a7ea4d434638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(user_query, inputs=\"\", sep=\"\\n\\n### \"):\n",
    "    sys_msg = \"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\"\n",
    "    p = sys_msg\n",
    "    roles = [\"指示\", \"応答\"]\n",
    "    msgs = [\": \\n\" + user_query, \": \"]\n",
    "    if inputs:\n",
    "        roles.insert(1, \"入力\")\n",
    "        msgs.insert(1, \": \\n\" + inputs)\n",
    "    for role, msg in zip(roles, msgs):\n",
    "        p += sep + role + msg\n",
    "    return p\n",
    "\n",
    "# Infer with prompt without any additional input\n",
    "user_inputs = {\n",
    "    \"user_query\": \"VR とはどのようなものですか？\",\n",
    "    \"inputs\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "028ee3a0-06b5-4c96-a442-56dc216b3e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# KV cache input/output name for models that is converted with optimum-intel.\\no_past_kv_names = []\\nfor n in range(32):\\n    past_kv_name = f'present.{n}.key'\\n    o_past_kv_names.append(past_kv_name)\\n    past_kv_name = f'present.{n}.value'\\n    o_past_kv_names.append(past_kv_name)\\n\\ni_past_kv_names = []\\nfor n in range(32):\\n    past_kv_name = f'past_key_values.{n}.key'\\n    i_past_kv_names.append(past_kv_name)\\n    past_kv_name = f'past_key_values.{n}.value'\\n    i_past_kv_names.append(past_kv_name)\\n\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = build_prompt(**user_inputs)\n",
    "\n",
    "# Tokenize the input text (text -> token IDs)\n",
    "# - The model input for the 1st iteration\n",
    "tokens = tokenizer(\n",
    "    prompt, \n",
    "    add_special_tokens=False, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "num_tokens     = tokens.input_ids.shape[-1]\n",
    "input_ids      = tokens.input_ids\n",
    "attention_mask = tokens.attention_mask\n",
    "position       = num_tokens\n",
    "position_ids   = np.array([[ n for n in range(position)]], dtype=np.int32)\n",
    "\n",
    "# Generates initial KV cache input\n",
    "seq_len = 0\n",
    "past_key_values = [ ov.Tensor(type=ov.Type.f32 , shape=(1,8,seq_len,128)) for _ in range(32 * 2) ]\n",
    "\n",
    "# generate lists that contains kv_cache output node names\n",
    "#\"\"\"\n",
    "# KV cache input/output name for models that is converted with OpenVINO convert_model() API.\n",
    "o_past_kv_names = []\n",
    "for n in range(32 * 2):\n",
    "    past_kv_name = str(107 + n)\n",
    "    o_past_kv_names.append(past_kv_name)\n",
    "i_past_kv_names = []\n",
    "for n in range(32 * 2):\n",
    "    past_kv_name = str(42 + n)\n",
    "    i_past_kv_names.append(past_kv_name)\n",
    "#\"\"\"\n",
    "    \n",
    "\"\"\"\n",
    "# KV cache input/output name for models that is converted with optimum-intel.\n",
    "o_past_kv_names = []\n",
    "for n in range(32):\n",
    "    past_kv_name = f'present.{n}.key'\n",
    "    o_past_kv_names.append(past_kv_name)\n",
    "    past_kv_name = f'present.{n}.value'\n",
    "    o_past_kv_names.append(past_kv_name)\n",
    "\n",
    "i_past_kv_names = []\n",
    "for n in range(32):\n",
    "    past_kv_name = f'past_key_values.{n}.key'\n",
    "    i_past_kv_names.append(past_kv_name)\n",
    "    past_kv_name = f'past_key_values.{n}.value'\n",
    "    i_past_kv_names.append(past_kv_name)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "75b240fb-d016-4e0a-8431-bc24192fd3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Start inferencing\n",
      "\n",
      "VR は、��想現実の略です。VR は、コンピューターの画面\n",
      "\n",
      "*** Completed.\n"
     ]
    }
   ],
   "source": [
    "print('*** Start inferencing')\n",
    "\n",
    "num_max_token_for_generation = 30\n",
    "generated_text_ids = []\n",
    "prev_output = ''\n",
    "\n",
    "#infer_request.reset_state()                                     # Initialize model internal state\n",
    "\n",
    "for token_count in range(num_max_token_for_generation):\n",
    "\n",
    "    # Run inference (to generate the logits for the next word prediction)\n",
    "    inputs={'input_ids'            : input_ids,\n",
    "            'attention_mask'       : attention_mask,\n",
    "            'position_ids'         : position_ids,\n",
    "    }\n",
    "    for n in range(64):\n",
    "        input_name = i_past_kv_names[n]\n",
    "        inputs[input_name] = past_key_values[n]\n",
    "\n",
    "    response = infer_request.infer(inputs)\n",
    "\n",
    "    # Sample the predicted token ID\n",
    "    logits = response['logits'][0, -1, :]\n",
    "    sampled_token_id = np.argmax(logits)                                                    # Greedy sampling\n",
    "\n",
    "    if sampled_token_id == tokenizer.eos_token_id:\n",
    "        print('\\n*** EOS token detected.')\n",
    "        break\n",
    "    # Display the text of the last generated portion\n",
    "    generated_text_ids = np.append(generated_text_ids, sampled_token_id).astype(np.int32)   # Append the predicted word to the bottom of the generated text ID array\n",
    "    output_text = tokenizer.decode(generated_text_ids)                                      # Decode and generate the text from the array of token IDs\n",
    "    print(output_text[len(prev_output):], end='', flush=True)                               # Print only the last generated word\n",
    "    prev_output = output_text\n",
    "\n",
    "    # Setup input data for the next iteration\n",
    "    input_ids      = np.array([[sampled_token_id]], dtype=np.int32)\n",
    "    attention_mask = np.array([[1]], dtype=np.int32)\n",
    "    position_ids   = np.array([[position]], dtype=np.int32)\n",
    "    position      += 1\n",
    "    past_key_values = [ response[o_past_kv_names[n]] for n in range(32 * 2) ]\n",
    "\n",
    "print(f'\\n\\n*** Completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12581e10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8935365-d6c1-413c-b47f-f7df4edf244f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f2dba-9e87-4b6c-afcb-c5daba38649a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
